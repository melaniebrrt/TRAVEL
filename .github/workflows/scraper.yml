name: Scraper Mensuel

on:
  schedule:
    - cron: '0 2 1 * *'  # 1er du mois √† 2h
  workflow_dispatch:      # Permet de lancer manuellement pour tester

permissions:
  contents: write         # ‚ö†Ô∏è OBLIGATOIRE pour pouvoir sauvegarder le CSV

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
      # 1Ô∏è‚É£ R√©cup√©rer le repo
      - uses: actions/checkout@v3

      # 2Ô∏è‚É£ Installer Python
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: 3.11

      # 3Ô∏è‚É£ Installer d√©pendances
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install --no-cache-dir -r requirements.txt
          pip install --no-cache-dir google-search-results

      # 4Ô∏è‚É£ Lancer le scraper
      - name: Run scraper + traduction
        env:
          SERPAPI_API_KEY: ${{ secrets.SERPAPI_API_KEY }}
        run: python event_pipeline_monthly_append.py

      # 5Ô∏è‚É£ (Optionnel) G√©n√©rer les embeddings
      # - name: Generate embeddings
      #   run: python generate_embeddings.py

      # 6Ô∏è‚É£ SAUVEGARDER LES R√âSULTATS (C'est √ßa qu'il manquait !)
      - name: Commit and Push changes
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "ü§ñ Update monthly events data"
          file_pattern: "data/*.csv data/*.json" # On ne sauvegarde que les donn√©es
